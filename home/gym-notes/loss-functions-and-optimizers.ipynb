{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions and optimizers\n",
    "\n",
    "\n",
    "## Basics\n",
    "\n",
    "The network doesn't only need to transform input to output, it needs a objective for that system. Here, we need to define a learning objective which a function that accepts two arguments:\n",
    "\n",
    "1. network's output\n",
    "2. The desired output\n",
    "\n",
    "The loss function's job is to return a single number, how close the network's prediction is to the desired output (loss value).\n",
    "\n",
    "Using the loss value, we calculate the gradients of the networks parameters and adjust them to decrease the loss value\n",
    "\n",
    "## loss functions\n",
    "\n",
    "Reside within pytorch module `nn`. Most common listed below\n",
    "\n",
    "`nn.MSELoss`: mean square error between arguments, which is the standard loss for regression problems\n",
    "\n",
    "`nn.BCELoss` and `nn.BCWEWithLogits`: Binary cross-entropy loss. When the output is a single probability value\n",
    "\n",
    "`nn.CrossEntropyLoss` and `nn.NLLLoss`: Maximum likelihood criteria that's used in multi-class classification problems\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "Goal is to take gradients of model parameters and change these parameters in order to decrease the loss value\n",
    "\n",
    "Reside in `torch.optim`. Common ones are\n",
    "\n",
    "`SGD`: vanilla stochastic gradient descent with an optional momentum extension\n",
    "\n",
    "`RMSprop`: an optimizer proposed by Geoffrey Hinton\n",
    "\n",
    "`Adagrad`: An adaptive gradient optimizer\n",
    "\n",
    "`Adam`: combination of `RMSprop` and `Adagrad`\n",
    "\n",
    "## Blueprint of training loop\n",
    "\n",
    "```\n",
    "for batch_x, batch_y in iterate_batches(data, batch_size=32): #1\n",
    "    batch_x_t = torch.tensor(batch_x)                           #2    \n",
    "    batch_y_t = torch.tensor(batch_y)                           #3    \n",
    "    out_t = net(batch_x_t)                                      #4    \n",
    "    loss_t = loss_function(out_t, batch_y_t).                   #5    \n",
    "    loss_t.backward()                                           #6    \n",
    "    optimizer.step()                                            #7    \n",
    "    optimizer.zero_grad()   \n",
    "```\n",
    "\n",
    "One full iteration over the dataset is called an *epoch*\n",
    "\n",
    "1. Iterate over dataset of certain batch size\n",
    "2. Convert batch x variables to tensor\n",
    "3. convert batch y to tensor\n",
    "4. feed the batch through the network\n",
    "5. Pass the input and output to the loss function\n",
    "6. Calculate the gradients using the `backward()` method, which remembers the graph and calculates the gradient for every leaf. Gradients accummulate in `tensor.grad`\n",
    "7. Apply the gradients\n",
    "8. Reset the gradients back to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
